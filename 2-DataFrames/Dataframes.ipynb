{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "german-collapse",
   "metadata": {},
   "source": [
    "# DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-librarian",
   "metadata": {},
   "source": [
    "DataFrame is a distributed collection of rows under named columns. In simple terms, it is same as a table in relational database or an Excel sheet with Column headers. It also shares some common characteristics with RDD:\n",
    "\n",
    "* Immutable in nature : We can create DataFrame / RDD once but canâ€™t change it. And we can transform a DataFrame / RDD after applying transformations.\n",
    "* Lazy Evaluations: Which means that a task is not executed until an action is performed.\n",
    "* Distributed: RDD and DataFrame both are distributed in nature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artistic-talent",
   "metadata": {},
   "source": [
    "### Advantages of the DataFrame\n",
    "* DataFrames are designed for processing large collection of structured or semi-structured data.\n",
    "* Observations in Spark DataFrame are organised under named columns, which helps Apache Spark to understand the schema of a DataFrame. This helps Spark optimize execution plan on these queries.\n",
    "* DataFrame in Apache Spark has the ability to handle petabytes of data.\n",
    "* DataFrame has a support for wide range of data format and sources.\n",
    "* It has API support for different languages like Python, R, Scala, Java.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-reconstruction",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sexual-inspiration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "appropriate-simple",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "attempted-median",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Basics').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "promotional-stupid",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('sales_info.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ambient-lawsuit",
   "metadata": {},
   "source": [
    ".show() the method used to show data in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "alternate-baghdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|     FB|   Carl|870.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-marketplace",
   "metadata": {},
   "source": [
    "Use `printSchema()` to show he schema of the data. Note, how tightly it is integrated to the SQL-like framework. You can even see that the schema accepts null values because nullable property is set True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "surgical-repair",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Person: string (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attended-secret",
   "metadata": {},
   "source": [
    "Fortunately a simple `columns` method exists to get column names back as a Python list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "attractive-redhead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Company', 'Person', 'Sales']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quantitative-shame",
   "metadata": {},
   "source": [
    "### Spark DataFrames have separate Column and Row types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "confident-fitting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['Company'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "false-movie",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.types.Row"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.head(2)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-mixture",
   "metadata": {},
   "source": [
    "### The `select` method to select particular columns\n",
    "\n",
    "We can use this method to actually select the DataFrame columns and see them. Note that we still have to use the `show` method to actually output the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "closing-balance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|Company|\n",
      "+-------+\n",
      "|   GOOG|\n",
      "|   GOOG|\n",
      "|   GOOG|\n",
      "|   MSFT|\n",
      "|   MSFT|\n",
      "|   MSFT|\n",
      "|     FB|\n",
      "|     FB|\n",
      "|   APPL|\n",
      "|   APPL|\n",
      "|   APPL|\n",
      "|   APPL|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Company').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecological-azerbaijan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|Company| Person|\n",
      "+-------+-------+\n",
      "|   GOOG|    Sam|\n",
      "|   GOOG|Charlie|\n",
      "|   GOOG|  Frank|\n",
      "|   MSFT|   Tina|\n",
      "|   MSFT|    Amy|\n",
      "|   MSFT|Vanessa|\n",
      "|     FB|   Carl|\n",
      "|     FB|  Sarah|\n",
      "|   APPL|   John|\n",
      "|   APPL|  Linda|\n",
      "|   APPL|   Mike|\n",
      "|   APPL|  Chris|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['Company','Person']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-argument",
   "metadata": {},
   "source": [
    "### The `limit` method to take first few rows (without any collection)\n",
    "\n",
    "Applying `limit()` to the DataFrame will result in a new Dataframe. This is a transformation and does not perform collecting the data. Other similar methods like `take` and `head` result in an Array of Rows i.e. a Scala Collection Object like `scala.collection.immutable.Array` (which is transformed to Python list while using the PySpark API)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "statewide-release",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-spelling",
   "metadata": {},
   "source": [
    "### The `head()` and the `asDict()` methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "laughing-delta",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Company='GOOG', Person='Sam', Sales=200.0),\n",
       " Row(Company='GOOG', Person='Charlie', Sales=120.0)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "underlying-faculty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Company': 'GOOG', 'Person': 'Sam', 'Sales': 200.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicti=df.head(2)[0].asDict()\n",
    "dicti"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "major-effectiveness",
   "metadata": {},
   "source": [
    "\n",
    "### The `count` method - number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "alert-pavilion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documented-bobby",
   "metadata": {},
   "source": [
    "### The `describe` and `summary` methods\n",
    "\n",
    "Similar to Pandas, the `describe` method is used for the statistical summary. But unlike Pandas, calling only `describe()` returns a DataFrame! This is due to the lazy evaluation - the actual computation is delayed as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "advance-reviewer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+------------------+\n",
      "|summary|Company| Person|             Sales|\n",
      "+-------+-------+-------+------------------+\n",
      "|  count|     12|     12|                12|\n",
      "|   mean|   null|   null| 360.5833333333333|\n",
      "| stddev|   null|   null|250.08742410799007|\n",
      "|    min|   APPL|  Chris|             120.0|\n",
      "|    max|   MSFT|Vanessa|             870.0|\n",
      "+-------+-------+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "latter-colombia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+------------------+\n",
      "|summary|Company| Person|             Sales|\n",
      "+-------+-------+-------+------------------+\n",
      "|  count|     12|     12|                12|\n",
      "|   mean|   null|   null| 360.5833333333333|\n",
      "| stddev|   null|   null|250.08742410799007|\n",
      "|    min|   APPL|  Chris|             120.0|\n",
      "|    25%|   null|   null|             130.0|\n",
      "|    50%|   null|   null|             250.0|\n",
      "|    75%|   null|   null|             350.0|\n",
      "|    max|   MSFT|Vanessa|             870.0|\n",
      "+-------+-------+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-harassment",
   "metadata": {},
   "source": [
    "### Coutning rows with distinct values - `distinct` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "signal-individual",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|Company|\n",
      "+-------+\n",
      "|   APPL|\n",
      "|   GOOG|\n",
      "|     FB|\n",
      "|   MSFT|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Company').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "certain-promise",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
