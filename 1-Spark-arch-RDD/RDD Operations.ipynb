{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "equal-tennis",
   "metadata": {},
   "source": [
    "# What is RDD?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generic-polls",
   "metadata": {},
   "source": [
    "RDD stands for “Resilient Distributed Dataset”. It is the fundamental data structure of Apache Spark. RDD in Apache Spark is an immutable collection of objects which computes on the different node of the cluster.\n",
    "Decomposing the name RDD:\n",
    "\n",
    "    1.Resilient i.e. fault-tolerant with the help of RDD lineage graph(DAG) and so able to recompute missing or damaged partitions due to node failures.\n",
    "\n",
    "    2.Distributed, since Data resides on multiple nodes.\n",
    "\n",
    "    3.Dataset represents records of the data you work with. The user can load the data set externally which can be either JSON file, CSV file, text file or database via JDBC with no specific data structure.\n",
    "\n",
    "Hence, each and every dataset in RDD is logically partitioned across many servers so that they can be computed on different nodes of the cluster. RDDs are fault tolerant i.e. It posses self-recovery in the case of failure.\n",
    "\n",
    "There are three ways to create RDDs in Spark such as – Data in stable storage, other RDDs, and parallelizing already existing collection in driver program. One can also operate Spark RDDs in parallel with a low-level API that offers transformations and actions. We will study these Spark RDD Operations later in this section.\n",
    "\n",
    "Spark RDD can also be cached and manually partitioned. Caching is beneficial when we use RDD several times. And manual partitioning is important to correctly balance partitions. Generally, smaller partitions allow distributing RDD data more equally, among more executors. Hence, fewer partitions make the work easy.\n",
    "\n",
    "Programmers can also call a persist method to indicate which RDDs they want to reuse in future operations. Spark keeps persistent RDDs in memory by default, but it can spill them to disk if there is not enough RAM. Users can also request other persistence strategies, such as storing the RDD only on disk or replicating it across machines, through flags to persist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollow-terrorist",
   "metadata": {},
   "source": [
    "# Contents:\n",
    "    a.Creating RDD\n",
    "    b.Basic Operations:\n",
    "        1. .map(...)The method is applied to each element of the RDD and transformation is done \n",
    "        2. .filter(...)The method allows you to select elements of your dataset that fit specified criteria\n",
    "        3. .flatMap(...)The method works similarly to .map(...) but returns a flattened results instead of a list. \n",
    "        4. .distinct(...)The method returns a list of distinct values in a specified column.\n",
    "        5. .sample(...)The method returns a randomized sample from the dataset.\n",
    "        6. .take(...)  \n",
    "        7. .collect(...)  \n",
    "        8. .reduce(...)  \n",
    "        9. .count(...)  \n",
    "        10. .saveAsTextFile(...)  \n",
    "        11. .foreach(...) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-religion",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "passing-shock",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "yellow-garbage",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=SparkContext(\"local[*]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-mounting",
   "metadata": {},
   "source": [
    "# A. Creating RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "packed-water",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 4 5 7 4 8 1 7 7 3 5 4 8 8 1 1 1 7 5 4]\n"
     ]
    }
   ],
   "source": [
    "lst=np.random.randint(0,10,20)\n",
    "print(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upset-bangladesh",
   "metadata": {},
   "source": [
    "### What did we just do? We created a RDD? What is a RDD?\n",
    "![](https://i.stack.imgur.com/cwrMN.png)\n",
    "Spark revolves around the concept of a resilient distributed dataset (RDD), which is a **fault-tolerant collection of elements that can be operated on in parallel**. SparkContext manages the distributed data over the worker nodes through the cluster manager. \n",
    "\n",
    "There are two ways to create RDDs: \n",
    "* parallelizing an existing collection in your driver program, or \n",
    "* referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat.\n",
    "\n",
    "We created a RDD using the former approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parliamentary-auction",
   "metadata": {},
   "source": [
    "# `A` is a pyspark RDD object, we cannot access the elements directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "outdoor-conflict",
   "metadata": {},
   "outputs": [],
   "source": [
    "A=sc.parallelize(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "exact-metro",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "falling-wichita",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aging-phase",
   "metadata": {},
   "source": [
    "### Opposite to parallelization - `collect` brings all the distributed elements and returns them to the head node. <br><br>Note - this is a slow process, do not use it often. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ready-helen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 4, 5, 7, 4, 8, 1, 7, 7, 3, 5, 4, 8, 8, 1, 1, 1, 7, 5, 4]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-documentation",
   "metadata": {},
   "source": [
    "### How were the partitions created? Use `glom` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "academic-medicine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6],\n",
       " [4, 5],\n",
       " [7, 4],\n",
       " [8],\n",
       " [1, 7],\n",
       " [7, 3],\n",
       " [5],\n",
       " [4, 8],\n",
       " [8, 1],\n",
       " [1],\n",
       " [1, 7],\n",
       " [5, 4]]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-silly",
   "metadata": {},
   "source": [
    "# B. Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-joshua",
   "metadata": {},
   "source": [
    "### 1. `map` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "yellow-header",
   "metadata": {},
   "outputs": [],
   "source": [
    "B=A.map(lambda x:x*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "substantial-popularity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[36, 16, 25, 49, 16, 64, 1, 49, 49, 9, 25, 16, 64, 64, 1, 1, 1, 49, 25, 16]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-breathing",
   "metadata": {},
   "source": [
    "`map` operation with regular Python function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "metric-amino",
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(x):\n",
    "    return x*x*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "finished-trunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "C=A.map(square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "attempted-hayes",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[216,\n",
       " 64,\n",
       " 125,\n",
       " 343,\n",
       " 64,\n",
       " 512,\n",
       " 1,\n",
       " 343,\n",
       " 343,\n",
       " 27,\n",
       " 125,\n",
       " 64,\n",
       " 512,\n",
       " 512,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 343,\n",
       " 125,\n",
       " 64]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-found",
   "metadata": {},
   "source": [
    "### 2. `filter` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "annoying-discipline",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 4, 8, 4, 8, 8, 4]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.filter(lambda x:x%4==0).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-water",
   "metadata": {},
   "source": [
    "### 3. `flatmap` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "accessible-anxiety",
   "metadata": {},
   "outputs": [],
   "source": [
    "D=A.flatMap(lambda x:(x,x*x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-regard",
   "metadata": {},
   "source": [
    "### `flatmap` method returns a new RDD by first applying a function to all elements of this RDD, and then flattening the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "divided-minimum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6,\n",
       " 36,\n",
       " 4,\n",
       " 16,\n",
       " 5,\n",
       " 25,\n",
       " 7,\n",
       " 49,\n",
       " 4,\n",
       " 16,\n",
       " 8,\n",
       " 64,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 49,\n",
       " 7,\n",
       " 49,\n",
       " 3,\n",
       " 9,\n",
       " 5,\n",
       " 25,\n",
       " 4,\n",
       " 16,\n",
       " 8,\n",
       " 64,\n",
       " 8,\n",
       " 64,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 49,\n",
       " 5,\n",
       " 25,\n",
       " 4,\n",
       " 16]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-portal",
   "metadata": {},
   "source": [
    "### 4. `distinct` function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-patio",
   "metadata": {},
   "source": [
    "### The method `RDD.distinct()` Returns a new dataset that contains the distinct elements of the source dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "binary-hamilton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 4, 5, 6, 7, 8]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-vancouver",
   "metadata": {},
   "source": [
    "### 5. `sample` function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-simon",
   "metadata": {},
   "source": [
    "## Sampling an RDD\n",
    "* RDDs are often very large.\n",
    "* **Aggregates, such as averages, can be approximated efficiently by using a sample.** This comes handy often for operation with extremely large datasets where a sample can tell a lot about the pattern and descriptive statistics of the data.\n",
    "* Sampling is done in parallel and requires limited computation.\n",
    "\n",
    "The method `RDD.sample(withReplacement,p)` generates a sample of the elements of the RDD. where\n",
    "- `withReplacement` is a boolean flag indicating whether or not a an element in the RDD can be sampled more than once.\n",
    "- `p` is the probability of accepting each element into the sample. Note that as the sampling is performed independently in each partition, the number of elements in the sample changes from sample to sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "precise-staff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample1= [7, 4, 4, 8, 1, 5]\n",
      "sample2= [7, 3, 8]\n"
     ]
    }
   ],
   "source": [
    "m=5\n",
    "n=20\n",
    "print('sample1=',A.sample(False,m/n).collect()) \n",
    "print('sample2=',A.sample(False,m/n).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "consecutive-mauritius",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-mexico",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "center-fantasy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
